---
title: "RandomSearch GEO blood"
author: "Carlos Ruiz"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

## Introduction

In this report, we present the results from the hyperparameters optimization using RandomSearch. We tested four aspects of the network: number of CNN filters, kernel of CNN filters, stride of CNN filters and number of nodes of the dense network. We samples 50 combinations of these hyperparameters using 5-fold CV. Each model was run for 4 epochs.

```{r}
library(tidyverse)
```
## Results

Reading results

```{r}
random_res1 <- read_table2("../results/GSE55763_model/2021-06-09/results_50iter_round1.tsv", 
                          col_names = FALSE ) 
random_res2 <- read_table2("../results/GSE55763_model/2021-06-09/results_50iter_round2.tsv", 
                          col_names = FALSE ) 
random_res3 <- read_table2("../results/GSE55763_model/2021-06-09/results_50iter_round3.tsv", 
                          col_names = FALSE ) 
random_res <- cbind(random_res1, random_res2[, -1], random_res3[, -1])
random_mat <- t(random_res[-12, -1])
class(random_mat) <- "numeric"
colnames(random_mat) <- random_res$X1[-12]
random_df <- data.frame(random_mat) %>%
  tibble()
```
We will explore the correlation between each variable with the model score, computed as negative logistic loss. Thus, higher negative logistic loss are associated with better models.

### Number of CNN filters

```{r}
ggplot(random_df, aes(x = param_filters, y = mean_test_score)) +
  geom_point(position = "jitter") +  geom_smooth(method=lm) +
  theme_bw()
summary(lm(param_filters ~ mean_test_score, random_df))
```

The best results are obtained with few CNN filters, although there is high heterogeneity.

### Kernel size

This hyperparameter defines how many CpGs are included in each CNN filter.

```{r}
ggplot(random_df, aes(x = param_kernel_size, y = mean_test_score)) +
  geom_point(position = "jitter") +  geom_smooth(method=lm) +
  theme_bw()
summary(lm(param_kernel_size ~ mean_test_score, random_df))
```
 
Shorter kernel sizes achieved better accuracies.

### Stride 

This hyperparamter measures the degree of overlap between the CNN filter. 

```{r}
ggplot(random_df, aes(x = param_stride, y = mean_test_score)) +
  geom_point() +  geom_smooth(method=lm) +
  theme_bw()
summary(lm(param_stride ~ mean_test_score, random_df))
```

Best results are obtained for strides between 5 and 10.

### Pooling window 

This hyperparamter measures the degree of overlap between the CNN filter. 

```{r}
ggplot(random_df, aes(x = param_pool, y = mean_test_score)) +
  geom_point() +  geom_smooth(method=lm) +
  theme_bw()
summary(lm(param_pool ~ mean_test_score random_df))
```

There is no correlation between the window pooling and the score. Though, the best results are obtained for a pooling of 2. 

### Number of nodes in DNN

```{r}
ggplot(random_df, aes(x = param_dense_layer_sizes, y = mean_test_score)) +
  geom_point(position = "jitter") +  geom_smooth(method=lm) +
  theme_bw()
summary(lm(param_dense_layer_sizes ~ mean_test_score, random_df))
```

The best results are obtained for DNN of 128.

### Number of filters + Kernel size

```{r}
random_df %>%
  mutate(rank_Q = cut(rank_test_score, 
                              breaks = quantile(rank_test_score, seq(0, 1, 0.1)),
                              labels = paste0("Q", 1:10),
                              include.lowest = TRUE)) %>%
  ggplot(aes(x = param_filters, y = param_kernel_size, color = rank_Q)) +
 geom_point(position = "jitter")  +  
  theme_bw()
```

When the number of nodes and the kernel size are combined, the better results are obtained for short kernels and few filters.


```{r}
random_df %>%
  filter(param_filters < 10) %>%
ggplot(aes(x = param_kernel_size, y = mean_test_score)) +
  geom_point(position = "jitter") +  geom_smooth(method=lm) +
  theme_bw()
```
When restricting the combinations to a less than 10 filters, some shorter kernels achieved better performance.

```{r}
random_df %>%
  filter(param_kernel_size < 300) %>%
ggplot(aes(x = param_filters, y = mean_test_score)) +
  geom_point(position = "jitter") +  geom_smooth(method=lm) +
  theme_bw()
```

When restricting the models to kernel sizes lower than 250, best performance was achieved for few filters.

# Results in optimal kernel size and number of filters layers

We repeated the correlations between the score and the parameters by selection the models with better values of kernel size and DNN layers, the two hyperparameters with a higher influence on the score.

### Number of DNN nodes

```{r}
random_df %>%
  filter(param_filters < 10) %>%
  filter(param_kernel_size < 300) %>%
  ggplot(aes(x = param_dense_layer_sizes, y = mean_test_score)) +
    geom_point(position = "jitter") +  geom_smooth(method=lm) +
    theme_bw()
```

We obtained the best results with few or hundreds of nodes in the DNN layer. 

### Stride 

This hyperparamter measures the degree of overlap between the CNN filter. 

```{r}
random_df %>%
  filter(param_filters < 10) %>%
  filter(param_kernel_size < 300) %>%
ggplot(aes(x = param_stride, y = mean_test_score)) +
  geom_point() +  geom_smooth(method=lm) +
  theme_bw()
```

We do not observe clear differences due to stride.

### Pooling window 

This hyperparamter measures the degree of overlap between the CNN filter. 

```{r}
random_df %>%
  filter(param_filters < 10) %>%
  filter(param_kernel_size < 300) %>%
ggplot(aes(x = param_pool, y = mean_test_score)) +
  geom_point() +  geom_smooth(method=lm) +
  theme_bw()
```
Smaller polling seem to have better performance.