---
title: "Random Search CNN autoencode v1"
author: "Carlos Ruiz"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

## Introduction

In this report, we present the results from the hyperparameters optimization using RandomSearch. We tested four aspects of the network: number of CNN filters, kernel of CNN filters, stride of CNN filters and number of nodes of the dense network. We samples 50 combinations of these hyperparameters using 5-fold CV. Each model was run for 4 epochs.

```{r}
library(tidyverse)
```
## Results

Reading results

```{r}
random_res1 <- read_table2("../results/CNN_autoencod1D/2021-10-27/random_search/v1/CNN_autoencod1D_results_iter.tsv", 
                          col_names = FALSE ) 
random_res2 <- read_table2("../results/CNN_autoencod1D/2021-10-27/random_search/v2/CNN_autoencod1D_results_iter.tsv", 
                          col_names = FALSE ) 
random_res <- cbind(random_res1, random_res2[, -1])
random_mat <- t(random_res[-14, -1])
class(random_mat) <- "numeric"
colnames(random_mat) <- random_res$X1[-14]
random_df <- data.frame(random_mat) %>%
  tibble()
```
We will explore the correlation between each variable with the model score, computed as negative logistic loss. Thus, higher negative logistic loss are associated with better models.

### Number of CNN filters

```{r}
ggplot(random_df, aes(x = param_filters, y = mean_test_score)) +
  geom_point(position = "jitter") +  geom_smooth(method=lm) +
  theme_bw()
summary(lm(param_filters ~ mean_test_score, random_df))
```

There is no correlation between the number of CNN filters and the model score. 

### Kernel size

This hyperparameter defines how many CpGs are included in each CNN filter.

```{r}
ggplot(random_df, aes(x = param_kernel_size, y = mean_test_score)) +
  geom_point(position = "jitter") +  geom_smooth(method=lm) +
  theme_bw()
summary(lm(param_kernel_size ~ mean_test_score, random_df))
```
 
 We have obtained good performances with short and long kernels.

### Stride 

This hyperparamter measures the degree of overlap between the CNN filter. 

```{r}
ggplot(random_df, aes(x = param_stride, y = mean_test_score)) +
  geom_point() +  geom_smooth(method=lm) +
  theme_bw()
summary(lm(param_stride ~ mean_test_score, random_df))
```

We do not see substantial difference in score between the stride, although very short stride showed the worse performances.

### Pooling window 

This hyperparamter measures the degree of overlap between the CNN filter. 

```{r}
ggplot(random_df, aes(x = param_pool, y = mean_test_score)) +
  geom_point() +  geom_smooth(method=lm) +
  theme_bw()
summary(lm(param_pool ~ mean_test_score, random_df))
```

There is no correlation between the window pooling and the score.

### Number of nodes in DNN 1

```{r}
ggplot(random_df, aes(x = param_dense_layer_sizes1, y = mean_test_score)) +
  geom_point(position = "jitter") +  geom_smooth(method=lm) +
  theme_bw()
summary(lm(param_dense_layer_sizes1 ~ mean_test_score, random_df))
```

There are no big differences in the performance, due to the size of the first DNN layer.

### Number of nodes in DNN 2

```{r}
ggplot(random_df, aes(x = param_dense_layer_sizes2, y = mean_test_score)) +
  geom_point(position = "jitter") +  geom_smooth(method=lm) +
  theme_bw()
summary(lm(param_dense_layer_sizes2 ~ mean_test_score, random_df))
```

There are no big differences in the performance, due to the size of the second DNN layer.