---
title: "RandomSearch"
author: "Carlos Ruiz"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

## Introduction

In this report, we present the results from the hyperparameters optimization using RandomSearch. We tested four aspects of the network: number of CNN filters, kernel of CNN filters, stride of CNN filters and number of nodes of the dense network. We samples 50 combinations of these hyperparameters using 5-fold CV. Each model was run for 4 epochs.

```{r}
library(tidyverse)
```
## Results

Reading results

```{r}
random_res1 <- read_table2("../results/tcga_model/2021-05-26/results_50iter_round1.tsv", 
                          col_names = FALSE ) 
random_res2 <- read_table2("../results/tcga_model/2021-05-26/results_50iter_round2.tsv", 
                          col_names = FALSE ) 
random_res <- cbind(random_res1, random_res2[, -1])
random_mat <- t(random_res[-12, -1])
class(random_mat) <- "numeric"
colnames(random_mat) <- random_res$X1[-12]
random_df <- data.frame(random_mat) %>%
  tibble()
```
We will explore the correlation between each variable with the model score, computed as negative logistic loss. Thus, higher negative logistic loss are associated with better models.

### Number of CNN filters

```{r}
ggplot(random_df, aes(x = param_filters, y = mean_test_score)) +
  geom_point(position = "jitter") +  geom_smooth(method=lm) +
  theme_bw()
summary(lm(param_filters ~ mean_test_score, random_df))
```

There is no correlation between the number of CNN filters and the model score. 

### Kernel size

This hyperparameter defines how many CpGs are included in each CNN filter.

```{r}
ggplot(random_df, aes(x = param_kernel_size, y = mean_test_score)) +
  geom_point(position = "jitter") +  geom_smooth(method=lm) +
  theme_bw()
summary(lm(param_kernel_size ~ mean_test_score, random_df))
```
 
 We have obtained good performances with short and long kernels.

### Stride 

This hyperparamter measures the degree of overlap between the CNN filter. 

```{r}
ggplot(random_df, aes(x = param_stride, y = mean_test_score)) +
  geom_point() +  geom_smooth(method=lm) +
  theme_bw()
summary(lm(param_stride ~ mean_test_score, random_df))
```

We do not see substantial difference in score between the stride, although very short stride showed the worse performances.

### Pooling window 

This hyperparamter measures the degree of overlap between the CNN filter. 

```{r}
ggplot(random_df, aes(x = param_pool, y = mean_test_score)) +
  geom_point() +  geom_smooth(method=lm) +
  theme_bw()
summary(lm(param_pool ~ mean_test_score, random_df))
```

There is no correlation between the window pooling and the score.

### Number of nodes in DNN

```{r}
ggplot(random_df, aes(x = param_dense_layer_sizes, y = mean_test_score)) +
  geom_point(position = "jitter") +  geom_smooth(method=lm) +
  theme_bw()
summary(lm(param_dense_layer_sizes ~ mean_test_score, random_df))
```

The most consistent performances were achieved for dense layer sizes of 256. Until 64, we achieved good performance but then it dropped.


### Number of nodes in DNN + Kernel size

```{r}
random_df %>%
  mutate(rank_Q = cut(rank_test_score, 
                              breaks = quantile(rank_test_score, seq(0, 1, 0.2)),
                              labels = paste0("Q", 1:5),
                              include.lowest = TRUE)) %>%
  ggplot(aes(x = param_dense_layer_sizes, y = param_kernel_size, color = rank_Q)) +
 geom_point(position = "jitter")  +  
  theme_bw()
```

When the number of nodes and the kernel size are combined, the better results are obtained for a layer size of 256 and short kernels or for a layer size of 64 and long kernels.


```{r}
random_df %>%
  filter(param_dense_layer_sizes == 256) %>%
ggplot(aes(x = param_kernel_size, y = mean_test_score)) +
  geom_point(position = "jitter") +  geom_smooth(method=lm) +
  theme_bw()
```
When restricting the combinations to a layer size of 256, shorter kernels achieved better performance.

```{r}
random_df %>%
  filter(param_kernel_size > 16) %>%
ggplot(aes(x = param_dense_layer_sizes, y = mean_test_score)) +
  geom_point(position = "jitter") +  geom_smooth(method=lm) +
  theme_bw()
```

When restricting the models to kernel sizes higher than 16, we do not observe big differences between the scores of dense layers >= 64.

# Results in optimal kernel size and DNN layers

We repeated the correlations between the score and the parameters by selection the models with better values of kernel size and DNN layers, the two hyperparameters with a higher influence on the score.

### Number of CNN filters

```{r}
random_df %>%
  filter(param_kernel_size > 16) %>%
  filter(param_dense_layer_sizes == 256) %>%
  ggplot(aes(x = param_filters, y = mean_test_score)) +
    geom_point(position = "jitter") +  geom_smooth(method=lm) +
    theme_bw()
```

Better values are obtained for a number of CNN filters => 16.

### Stride 

This hyperparamter measures the degree of overlap between the CNN filter. 

```{r}
random_df %>%
  filter(param_kernel_size > 16) %>%
  filter(param_dense_layer_sizes == 256) %>%
ggplot(aes(x = param_stride, y = mean_test_score)) +
  geom_point() +  geom_smooth(method=lm) +
  theme_bw()
```

We do not observe clear differences due to stride.

### Pooling window 

This hyperparamter measures the degree of overlap between the CNN filter. 

```{r}
random_df %>%
  filter(param_kernel_size > 16) %>%
  filter(param_dense_layer_sizes == 256) %>%
ggplot(aes(x = param_pool, y = mean_test_score)) +
  geom_point() +  geom_smooth(method=lm) +
  theme_bw()
```

We do not see big differences due to pooling.